{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-guess",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-cover",
   "metadata": {},
   "outputs": [],
   "source": [
    "!code ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-circulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-likelihood",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-capacity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DatasetHandler import DatasetHandler\n",
    "training_handler = DatasetHandler('dataset/training')\n",
    "validation_handler = DatasetHandler('dataset/validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-fluid",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dataset classes')\n",
    "print('\\t', training_handler.classes)\n",
    "\n",
    "print('Dataset dimension')\n",
    "print('\\t', len(training_handler.s2_paths), 'training samples belonging to ', len(training_handler.classes), 'classes')\n",
    "print('\\t', len(validation_handler.s2_paths), 'validation samples belonging to ', len(validation_handler.classes), 'classes')\n",
    "\n",
    "print('Dataset example')\n",
    "idx = 0\n",
    "print('\\t Sentinel-2 image path', training_handler.s2_paths[idx], '\\n \\t Sentinel-2 image label', training_handler.s2_labels[idx])#, '\\n \\t Sentinel-2 image shape', training_handler.s2_shape) \n",
    "print('\\n \\t Sentinel-1 image path', training_handler.s1_paths[idx], '\\n \\t Sentinel-1 image label', training_handler.s1_labels[idx])#, '\\n \\t Sentinel-1 image shape', training_handler.s1_shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-cliff",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = []\n",
    "for c in training_handler.classes:\n",
    "    classes.append(c.split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-ground",
   "metadata": {},
   "source": [
    "## Sentinel-2 Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impaired-retirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2classifier = None\n",
    "from CNN_Classifier import CNN_Classifier\n",
    "s2classifier = CNN_Classifier((64,64, 12), 5)\n",
    "\n",
    "s2classifier.model = load_model('weights/S2-classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-tragedy",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "training_loader = training_handler.s2_data_loader(batch_size, (64,64,12))\n",
    "validation_loader = validation_handler.s2_data_loader(batch_size, (64,64,12))\n",
    "\n",
    "training_steps = 4*len(training_handler.s2_paths)\n",
    "validation_steps = 4*len(validation_handler.s2_paths)\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "s2classifier.train_model(epochs, batch_size, training_loader, validation_loader, training_steps, validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-tourist",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2classifier.model.save('weights/S2-classifier.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-alcohol",
   "metadata": {},
   "source": [
    "## Sentinel-1 classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-tampa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DatasetHandler import DatasetHandler\n",
    "training_handler = DatasetHandler('dataset/training')\n",
    "validation_handler = DatasetHandler('dataset/validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-congress",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1classifier = None\n",
    "from CNN_Classifier import CNN_Classifier\n",
    "s1classifier = CNN_Classifier((64,64, 2), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-kingdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1classifier.model = load_model('weights/S1-classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-fluid",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "training_loader = training_handler.s1_data_loader(batch_size, (64,64,2))\n",
    "validation_loader = validation_handler.s1_data_loader(batch_size, (64,64,2))\n",
    "\n",
    "training_steps = 4*len(training_handler.s1_paths)\n",
    "validation_steps = 4*len(validation_handler.s1_paths)\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "s1classifier.train_model(epochs, batch_size, training_loader, validation_loader, training_steps, validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-jurisdiction",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s1classifier.model.save('weights/S1-classifier.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-exemption",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Early Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-italian",
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyclassifier = None\n",
    "from CNN_Classifier import CNN_Classifier\n",
    "earlyclassifier = CNN_Classifier((64,64, 12+2), 5)\n",
    "\n",
    "earlyclassifier.model = load_model('weights/S2-S1-early-classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-silence",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "training_loader = training_handler.s2_s1_data_loader_2(batch_size, (64,64,12), (64,64,2))\n",
    "validation_loader = validation_handler.s2_s1_data_loader_2(batch_size, (64,64,12), (64,64,2))\n",
    "\n",
    "training_steps = 4*len(training_handler.s2_paths)\n",
    "validation_steps = 4*len(validation_handler.s2_paths)\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "earlyclassifier.train_model(epochs, batch_size, training_loader, validation_loader, training_steps, validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-figure",
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyclassifier.model.save('weights/S2-S1-early-classifier.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-sampling",
   "metadata": {},
   "source": [
    "# Joint Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-butter",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from DatasetHandler import DatasetHandler\n",
    "training_handler = DatasetHandler('dataset/training')\n",
    "validation_handler = DatasetHandler('dataset/validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-cardiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fclassifier = None\n",
    "from Fusion_Classifier import Fusion_Classifier\n",
    "fclassifier = Fusion_Classifier((64,64, 12), (64,64,2), 5)\n",
    "\n",
    "fclassifier.model = load_model('weights/S2-S1-classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-cathedral",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "training_loader = training_handler.s2_s1_data_loader(batch_size, (64,64,12), (64,64,2))\n",
    "validation_loader = validation_handler.s2_s1_data_loader(batch_size, (64,64,12), (64,64,2))\n",
    "\n",
    "training_steps = 4*len(training_handler.s1_paths)\n",
    "validation_steps = 4*len(validation_handler.s1_paths)\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "fclassifier.train_model(epochs, batch_size, training_loader, validation_loader, training_steps, validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-guitar",
   "metadata": {},
   "outputs": [],
   "source": [
    "fclassifier.model.save('weights/S2-S1-classifier.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-closing",
   "metadata": {},
   "source": [
    "# Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-census",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-palestinian",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DatasetHandler import DatasetHandler\n",
    "training_handler = DatasetHandler('dataset/training')\n",
    "validation_handler = DatasetHandler('dataset/validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-monaco",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = []\n",
    "for c in training_handler.classes:\n",
    "    classes.append(c.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-electric",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "s2classifier = CNN_Classifier((64,64, 12), 5)\n",
    "s1classifier = CNN_Classifier((64,64, 2), 5)\n",
    "fclassifier = Fusion_Classifier((64,64, 12), (64,64,2), 5)\n",
    "earlyclassifier = CNN_Classifier((64,64, 12+2), 5)\n",
    "\n",
    "s2classifier.model = load_model('weights/S2-classifier.h5')\n",
    "s1classifier.model = load_model('weights/S1-classifier.h5')\n",
    "fclassifier.model = load_model('weights/S2-S1-classifier.h5')\n",
    "earlyclassifier.model = load_model('weights/S2-S1-early-classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-wisconsin",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_loader = validation_handler.s2_s1_data_loader(10*len(validation_handler.s1_paths), (64,64,12), (64,64,2))\n",
    "s2_s1, g_truth = next(iter(validation_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-bahamas",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_pre = s2classifier.model.predict(s2_s1[0])\n",
    "s1_pre = s1classifier.model.predict(s2_s1[1])\n",
    "f_pre = fclassifier.model.predict(s2_s1)\n",
    "\n",
    "s2s1 = np.zeros((10*len(validation_handler.s1_paths), 64,64, 14))\n",
    "s2s1[...,0:2] = s2_s1[1]\n",
    "s2s1[...,2:] = s2_s1[0]\n",
    "\n",
    "e_pre = earlyclassifier.model.predict(s2s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-seeking",
   "metadata": {},
   "outputs": [],
   "source": [
    "late_sum = []\n",
    "for i in range(s2_pre.shape[0]):\n",
    "        late_sum.append(np.argmax((s1_pre[i]+s2_pre[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-seventh",
   "metadata": {},
   "outputs": [],
   "source": [
    "late_weight = []\n",
    "w1 = np.array([0, 1, 1, 1, 0])\n",
    "w2 = 1 - w1\n",
    "\n",
    "for i in range(s2_pre.shape[0]):\n",
    "        late_weight.append(np.argmax((w1*s1_pre[i]+w2*s2_pre[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-sentence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 20}\n",
    "\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-utilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows = 1, ncols = 1, figsize = (12,10))\n",
    "maxs = []\n",
    "\n",
    "# S2\n",
    "ground_truth = np.argmax(g_truth, axis = 1)\n",
    "prediction = np.argmax(s2_pre, axis = 1)\n",
    "cm = confusion_matrix(ground_truth, prediction, normalize='true')\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=classes)\n",
    "cmd.plot(ax=axes, cmap='Blues')\n",
    "print('S2')\n",
    "print('Accuracy:', cm.diagonal(), 'mean: ', cm.diagonal().mean())\n",
    "print(classification_report(ground_truth, prediction, target_names=classes, digits=4))\n",
    "axes.get_images()[0].set_clim(0, 1)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# S1\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = 1, figsize = (12,10))\n",
    "ground_truth = np.argmax(g_truth, axis = 1)\n",
    "prediction = np.argmax(s1_pre, axis = 1)\n",
    "cm = confusion_matrix(ground_truth, prediction, normalize='true')\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=classes)\n",
    "cmd.plot(ax=axes, cmap='Blues')\n",
    "print('S1')\n",
    "print('Accuracy:', cm.diagonal(), 'mean: ', cm.diagonal().mean())\n",
    "print(classification_report(ground_truth, prediction, target_names=classes, digits=4))\n",
    "axes.get_images()[0].set_clim(0, 1)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Joint Fusion\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = 1, figsize = (12,10))\n",
    "ground_truth = np.argmax(g_truth, axis = 1)\n",
    "prediction = np.argmax(f_pre, axis = 1)\n",
    "cm = confusion_matrix(ground_truth, prediction, normalize='true')\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=classes)\n",
    "cmd.plot(ax=axes, cmap='Blues')\n",
    "print('Joint Fusion')\n",
    "print('Accuracy:', cm.diagonal(), 'mean: ', cm.diagonal().mean())\n",
    "print(classification_report(ground_truth, prediction, target_names=classes, digits=4))\n",
    "axes.get_images()[0].set_clim(0, 1)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Late fusion sum\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = 1, figsize = (12,10))\n",
    "cm = confusion_matrix(ground_truth, late_sum, normalize='true')\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=classes)\n",
    "cmd.plot(ax=axes, cmap='Blues')\n",
    "print('Late Fusion')\n",
    "print('Accuracy:', cm.diagonal(), 'mean: ', cm.diagonal().mean())\n",
    "print(classification_report(ground_truth, late_sum, target_names=classes, digits=4))\n",
    "axes.get_images()[0].set_clim(0, 1)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Late fusion weight\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = 1, figsize = (12,10))\n",
    "cm = confusion_matrix(ground_truth, late_weight, normalize='true')\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=classes)\n",
    "cmd.plot(ax=axes, cmap='Blues')\n",
    "print('Late Fusion')\n",
    "print('Accuracy:', cm.diagonal(), 'mean: ', cm.diagonal().mean())\n",
    "print(classification_report(ground_truth, late_weight, target_names=classes, digits=4))\n",
    "axes.get_images()[0].set_clim(0, 1)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Early Fusion\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = 1, figsize = (12,10))\n",
    "ground_truth = np.argmax(g_truth, axis = 1)\n",
    "prediction = np.argmax(e_pre, axis = 1)\n",
    "cm = confusion_matrix(ground_truth, prediction, normalize='true')\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=classes)\n",
    "cmd.plot(ax=axes, cmap='Blues')\n",
    "print('Early Fusion')\n",
    "print('Accuracy:', cm.diagonal(), 'mean: ', cm.diagonal().mean())\n",
    "print(classification_report(ground_truth, prediction, target_names=classes, digits=4))\n",
    "axes.get_images()[0].set_clim(0, 1)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-xerox",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
